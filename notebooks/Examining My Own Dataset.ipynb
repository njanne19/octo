{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "274f0f76-6a1b-4d7f-a715-137d9d075e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import json \n",
    "import os\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a02ace87-8eab-4bb5-88fd-717d8f112d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/nick/Documents/octo/octo/notebooks'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdd52f78-a213-4670-ba8f-fa9b21084ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = os.path.join('..', '..', 'tmp', 'cobot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aae2d28-76c9-4fae-a19a-106dbfae6560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Information:\n",
      "{\n",
      "  \"fileFormat\": \"tfrecord\",\n",
      "  \"name\": \"my_cobot_example\",\n",
      "  \"splits\": [\n",
      "    {\n",
      "      \"filepathTemplate\": \"{DATASET}-{SPLIT}.{FILEFORMAT}-{SHARD_INDEX}\",\n",
      "      \"name\": \"train\",\n",
      "      \"numBytes\": \"227029767\",\n",
      "      \"shardLengths\": [\n",
      "        \"1\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"version\": \"0.0.1\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(dataset_dir, 'dataset_info.json'), 'r') as f: \n",
    "    dataset_info = json.load(f) \n",
    "\n",
    "print(\"Dataset Information:\") \n",
    "print(json.dumps(dataset_info, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85078fb5-cc38-4fb6-ac1b-0c53640e281b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Information:\n",
      "{\n",
      "  \"featuresDict\": {\n",
      "    \"features\": {\n",
      "      \"steps\": {\n",
      "        \"pythonClassName\": \"tensorflow_datasets.core.features.dataset_feature.Dataset\",\n",
      "        \"sequence\": {\n",
      "          \"feature\": {\n",
      "            \"featuresDict\": {\n",
      "              \"features\": {\n",
      "                \"action\": {\n",
      "                  \"pythonClassName\": \"tensorflow_datasets.core.features.tensor_feature.Tensor\",\n",
      "                  \"tensor\": {\n",
      "                    \"dtype\": \"float64\",\n",
      "                    \"encoding\": \"zlib\",\n",
      "                    \"shape\": {\n",
      "                      \"dimensions\": [\n",
      "                        \"6\"\n",
      "                      ]\n",
      "                    }\n",
      "                  }\n",
      "                },\n",
      "                \"discount\": {\n",
      "                  \"pythonClassName\": \"tensorflow_datasets.core.features.tensor_feature.Tensor\",\n",
      "                  \"tensor\": {\n",
      "                    \"dtype\": \"float64\",\n",
      "                    \"encoding\": \"none\",\n",
      "                    \"shape\": {}\n",
      "                  }\n",
      "                },\n",
      "                \"is_first\": {\n",
      "                  \"pythonClassName\": \"tensorflow_datasets.core.features.tensor_feature.Tensor\",\n",
      "                  \"tensor\": {\n",
      "                    \"dtype\": \"bool\",\n",
      "                    \"encoding\": \"none\",\n",
      "                    \"shape\": {}\n",
      "                  }\n",
      "                },\n",
      "                \"is_last\": {\n",
      "                  \"pythonClassName\": \"tensorflow_datasets.core.features.tensor_feature.Tensor\",\n",
      "                  \"tensor\": {\n",
      "                    \"dtype\": \"bool\",\n",
      "                    \"encoding\": \"none\",\n",
      "                    \"shape\": {}\n",
      "                  }\n",
      "                },\n",
      "                \"is_terminal\": {\n",
      "                  \"pythonClassName\": \"tensorflow_datasets.core.features.tensor_feature.Tensor\",\n",
      "                  \"tensor\": {\n",
      "                    \"dtype\": \"bool\",\n",
      "                    \"encoding\": \"none\",\n",
      "                    \"shape\": {}\n",
      "                  }\n",
      "                },\n",
      "                \"observation\": {\n",
      "                  \"featuresDict\": {\n",
      "                    \"features\": {\n",
      "                      \"image_primary\": {\n",
      "                        \"pythonClassName\": \"tensorflow_datasets.core.features.tensor_feature.Tensor\",\n",
      "                        \"tensor\": {\n",
      "                          \"dtype\": \"uint8\",\n",
      "                          \"encoding\": \"zlib\",\n",
      "                          \"shape\": {\n",
      "                            \"dimensions\": [\n",
      "                              \"480\",\n",
      "                              \"640\",\n",
      "                              \"3\"\n",
      "                            ]\n",
      "                          }\n",
      "                        }\n",
      "                      },\n",
      "                      \"image_wrist\": {\n",
      "                        \"pythonClassName\": \"tensorflow_datasets.core.features.tensor_feature.Tensor\",\n",
      "                        \"tensor\": {\n",
      "                          \"dtype\": \"uint8\",\n",
      "                          \"encoding\": \"zlib\",\n",
      "                          \"shape\": {\n",
      "                            \"dimensions\": [\n",
      "                              \"480\",\n",
      "                              \"640\",\n",
      "                              \"3\"\n",
      "                            ]\n",
      "                          }\n",
      "                        }\n",
      "                      },\n",
      "                      \"proprio\": {\n",
      "                        \"pythonClassName\": \"tensorflow_datasets.core.features.tensor_feature.Tensor\",\n",
      "                        \"tensor\": {\n",
      "                          \"dtype\": \"float64\",\n",
      "                          \"encoding\": \"zlib\",\n",
      "                          \"shape\": {\n",
      "                            \"dimensions\": [\n",
      "                              \"6\"\n",
      "                            ]\n",
      "                          }\n",
      "                        }\n",
      "                      }\n",
      "                    }\n",
      "                  },\n",
      "                  \"pythonClassName\": \"tensorflow_datasets.core.features.features_dict.FeaturesDict\"\n",
      "                },\n",
      "                \"reward\": {\n",
      "                  \"pythonClassName\": \"tensorflow_datasets.core.features.tensor_feature.Tensor\",\n",
      "                  \"tensor\": {\n",
      "                    \"dtype\": \"float64\",\n",
      "                    \"encoding\": \"none\",\n",
      "                    \"shape\": {}\n",
      "                  }\n",
      "                }\n",
      "              }\n",
      "            },\n",
      "            \"pythonClassName\": \"tensorflow_datasets.core.features.features_dict.FeaturesDict\"\n",
      "          },\n",
      "          \"length\": \"-1\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"pythonClassName\": \"tensorflow_datasets.core.features.features_dict.FeaturesDict\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(dataset_dir, 'features.json'), 'r') as f: \n",
    "    features = json.load(f)\n",
    "\n",
    "print(\"Features Information:\") \n",
    "print(json.dumps(features, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c58fb45-8d22-4279-ae28-79fc45eace6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dir = os.path.abspath(os.path.join('..', '..', 'tmp'))\n",
    "dataset_builder = tfds.builder_from_directory(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "833df1f2-5ce2-4a24-89c0-5854a3d4c9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on ReadOnlyBuilder in module tensorflow_datasets.core.read_only_builder object:\n",
      "\n",
      "class ReadOnlyBuilder(tensorflow_datasets.core.dataset_builder.FileReaderBuilder)\n",
      " |  ReadOnlyBuilder(builder_dir: Union[str, os.PathLike], *, info_proto: Optional[dataset_info_pb2.DatasetInfo] = None)\n",
      " |  \n",
      " |  Generic DatasetBuilder loading from a directory.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ReadOnlyBuilder\n",
      " |      tensorflow_datasets.core.dataset_builder.FileReaderBuilder\n",
      " |      tensorflow_datasets.core.dataset_builder.DatasetBuilder\n",
      " |      tensorflow_datasets.core.registered.RegisteredDataset\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, builder_dir: Union[str, os.PathLike], *, info_proto: Optional[dataset_info_pb2.DatasetInfo] = None)\n",
      " |      Constructor.\n",
      " |      \n",
      " |      Args:\n",
      " |        builder_dir: Directory of the dataset to load (e.g.\n",
      " |          `~/tensorflow_datasets/mnist/3.0.0/`)\n",
      " |        info_proto: DatasetInfo describing the name, config, etc of the requested\n",
      " |          dataset. Note that this overwrites dataset info that may be present in\n",
      " |          builder_dir.\n",
      " |      \n",
      " |      Raises:\n",
      " |        FileNotFoundError: If the builder_dir does not exist.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  name = 'read_only_builder'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow_datasets.core.dataset_builder.DatasetBuilder:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  as_data_source(self, split: 'Optional[Tree[splits_lib.SplitArg]]' = None, *, decoders: 'Optional[TreeDict[decode.partial_decode.DecoderArg]]' = None) -> 'ListOrTreeOrElem[Sequence[Any]]'\n",
      " |      Constructs an `ArrayRecordDataSource`.\n",
      " |      \n",
      " |      Args:\n",
      " |        split: Which split of the data to load (e.g. `'train'`, `'test'`,\n",
      " |          `['train', 'test']`, `'train[80%:]'`,...). See our [split API\n",
      " |          guide](https://www.tensorflow.org/datasets/splits). If `None`, will\n",
      " |          return all splits in a `Dict[Split, Sequence]`.\n",
      " |        decoders: Nested dict of `Decoder` objects which allow to customize the\n",
      " |          decoding. The structure should match the feature structure, but only\n",
      " |          customized feature keys need to be present. See [the\n",
      " |          guide](https://github.com/tensorflow/datasets/blob/master/docs/decode.md)\n",
      " |          for more info.\n",
      " |      \n",
      " |      Returns:\n",
      " |        `Sequence` if `split`,\n",
      " |        `dict<key: tfds.Split, value: Sequence>` otherwise.\n",
      " |      \n",
      " |      Raises:\n",
      " |        NotImplementedError if the data was not generated using ArrayRecords.\n",
      " |  \n",
      " |  as_dataset(self, split: 'Optional[Tree[splits_lib.SplitArg]]' = None, *, batch_size: 'Optional[int]' = None, shuffle_files: 'bool' = False, decoders: 'Optional[TreeDict[decode.partial_decode.DecoderArg]]' = None, read_config: 'Optional[read_config_lib.ReadConfig]' = None, as_supervised: 'bool' = False)\n",
      " |      Constructs a `tf.data.Dataset`.\n",
      " |      \n",
      " |      Callers must pass arguments as keyword arguments.\n",
      " |      \n",
      " |      The output types vary depending on the parameters. Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      builder = tfds.builder('imdb_reviews')\n",
      " |      builder.download_and_prepare()\n",
      " |      \n",
      " |      # Default parameters: Returns the dict of tf.data.Dataset\n",
      " |      ds_all_dict = builder.as_dataset()\n",
      " |      assert isinstance(ds_all_dict, dict)\n",
      " |      print(ds_all_dict.keys())  # ==> ['test', 'train', 'unsupervised']\n",
      " |      \n",
      " |      assert isinstance(ds_all_dict['test'], tf.data.Dataset)\n",
      " |      # Each dataset (test, train, unsup.) consists of dictionaries\n",
      " |      # {'label': <tf.Tensor: .. dtype=int64, numpy=1>,\n",
      " |      #  'text': <tf.Tensor: .. dtype=string, numpy=b\"I've watched the movie ..\">}\n",
      " |      # {'label': <tf.Tensor: .. dtype=int64, numpy=1>,\n",
      " |      #  'text': <tf.Tensor: .. dtype=string, numpy=b'If you love Japanese ..'>}\n",
      " |      \n",
      " |      # With as_supervised: tf.data.Dataset only contains (feature, label) tuples\n",
      " |      ds_all_supervised = builder.as_dataset(as_supervised=True)\n",
      " |      assert isinstance(ds_all_supervised, dict)\n",
      " |      print(ds_all_supervised.keys())  # ==> ['test', 'train', 'unsupervised']\n",
      " |      \n",
      " |      assert isinstance(ds_all_supervised['test'], tf.data.Dataset)\n",
      " |      # Each dataset (test, train, unsup.) consists of tuples (text, label)\n",
      " |      # (<tf.Tensor: ... dtype=string, numpy=b\"I've watched the movie ..\">,\n",
      " |      #  <tf.Tensor: ... dtype=int64, numpy=1>)\n",
      " |      # (<tf.Tensor: ... dtype=string, numpy=b\"If you love Japanese ..\">,\n",
      " |      #  <tf.Tensor: ... dtype=int64, numpy=1>)\n",
      " |      \n",
      " |      # Same as above plus requesting a particular split\n",
      " |      ds_test_supervised = builder.as_dataset(as_supervised=True, split='test')\n",
      " |      assert isinstance(ds_test_supervised, tf.data.Dataset)\n",
      " |      # The dataset consists of tuples (text, label)\n",
      " |      # (<tf.Tensor: ... dtype=string, numpy=b\"I've watched the movie ..\">,\n",
      " |      #  <tf.Tensor: ... dtype=int64, numpy=1>)\n",
      " |      # (<tf.Tensor: ... dtype=string, numpy=b\"If you love Japanese ..\">,\n",
      " |      #  <tf.Tensor: ... dtype=int64, numpy=1>)\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        split: Which split of the data to load (e.g. `'train'`, `'test'`,\n",
      " |          `['train', 'test']`, `'train[80%:]'`,...). See our [split API\n",
      " |          guide](https://www.tensorflow.org/datasets/splits). If `None`, will\n",
      " |          return all splits in a `Dict[Split, tf.data.Dataset]`.\n",
      " |        batch_size: `int`, batch size. Note that variable-length features will be\n",
      " |          0-padded if `batch_size` is set. Users that want more custom behavior\n",
      " |          should use `batch_size=None` and use the `tf.data` API to construct a\n",
      " |          custom pipeline. If `batch_size == -1`, will return feature dictionaries\n",
      " |          of the whole dataset with `tf.Tensor`s instead of a `tf.data.Dataset`.\n",
      " |        shuffle_files: `bool`, whether to shuffle the input files. Defaults to\n",
      " |          `False`.\n",
      " |        decoders: Nested dict of `Decoder` objects which allow to customize the\n",
      " |          decoding. The structure should match the feature structure, but only\n",
      " |          customized feature keys need to be present. See [the\n",
      " |          guide](https://github.com/tensorflow/datasets/blob/master/docs/decode.md)\n",
      " |          for more info.\n",
      " |        read_config: `tfds.ReadConfig`, Additional options to configure the input\n",
      " |          pipeline (e.g. seed, num parallel reads,...).\n",
      " |        as_supervised: `bool`, if `True`, the returned `tf.data.Dataset` will have\n",
      " |          a 2-tuple structure `(input, label)` according to\n",
      " |          `builder.info.supervised_keys`. If `False`, the default, the returned\n",
      " |          `tf.data.Dataset` will have a dictionary with all the features.\n",
      " |      \n",
      " |      Returns:\n",
      " |        `tf.data.Dataset`, or if `split=None`, `dict<key: tfds.Split, value:\n",
      " |        tf.data.Dataset>`.\n",
      " |      \n",
      " |        If `batch_size` is -1, will return feature dictionaries containing\n",
      " |        the entire dataset in `tf.Tensor`s instead of a `tf.data.Dataset`.\n",
      " |  \n",
      " |  canonical_version = <functools.cached_property object>\n",
      " |  dataset_info_from_configs(self, **kwargs)\n",
      " |      Returns the DatasetInfo using given kwargs and config files.\n",
      " |      \n",
      " |      Sub-class should call this and add information not present in config files\n",
      " |      using kwargs directly passed to tfds.core.DatasetInfo object.\n",
      " |      \n",
      " |      If information is present both in passed arguments and config files, config\n",
      " |      files will prevail.\n",
      " |      \n",
      " |      Args:\n",
      " |        **kwargs: kw args to pass to DatasetInfo directly.\n",
      " |  \n",
      " |  download_and_prepare(self, *, download_dir: 'Optional[epath.PathLike]' = None, download_config: 'Optional[download.DownloadConfig]' = None, file_format: 'Optional[Union[str, file_adapters.FileFormat]]' = None) -> 'None'\n",
      " |      Downloads and prepares dataset for reading.\n",
      " |      \n",
      " |      Args:\n",
      " |        download_dir: directory where downloaded files are stored. Defaults to\n",
      " |          \"~/tensorflow-datasets/downloads\".\n",
      " |        download_config: `tfds.download.DownloadConfig`, further configuration for\n",
      " |          downloading and preparing dataset.\n",
      " |        file_format: optional `str` or `file_adapters.FileFormat`, format of the\n",
      " |          record files in which the dataset will be written.\n",
      " |      \n",
      " |      Raises:\n",
      " |        IOError: if there is not enough disk space available.\n",
      " |        RuntimeError: when the config cannot be found.\n",
      " |  \n",
      " |  get_default_builder_config(self) -> 'Optional[BuilderConfig]'\n",
      " |      Returns the default builder config if there is one.\n",
      " |      \n",
      " |      Note that for dataset builders that cannot use the `cls.BUILDER_CONFIGS`, we\n",
      " |      need a method that uses the instance to get `BUILDER_CONFIGS` and\n",
      " |      `DEFAULT_BUILDER_CONFIG_NAME`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        the default builder config if there is one\n",
      " |  \n",
      " |  get_reference(self, namespace: 'Optional[str]' = None) -> 'naming.DatasetReference'\n",
      " |      Returns a reference to the dataset produced by this dataset builder.\n",
      " |      \n",
      " |      Includes the config if specified, the version, and the data_dir that should\n",
      " |      contain this dataset.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        namespace: if this dataset is a community dataset, and therefore has a\n",
      " |          namespace, then the namespace must be provided such that it can be set\n",
      " |          in the reference. Note that a dataset builder is not aware that it is\n",
      " |          part of a namespace.\n",
      " |      \n",
      " |      Returns:\n",
      " |        a reference to this instantiated builder.\n",
      " |  \n",
      " |  is_prepared(self) -> 'bool'\n",
      " |      Returns whether this dataset is already downloaded and prepared.\n",
      " |  \n",
      " |  supported_versions = <functools.cached_property object>\n",
      " |  versions = <functools.cached_property object>\n",
      " |      Versions (canonical + availables), in preference order.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow_datasets.core.dataset_builder.DatasetBuilder:\n",
      " |  \n",
      " |  get_metadata() -> 'dataset_metadata.DatasetMetadata' from abc.ABCMeta\n",
      " |      Returns metadata (README, CITATIONS, ...) specified in config files.\n",
      " |      \n",
      " |      The config files are read from the same package where the DatasetBuilder has\n",
      " |      been defined, so those metadata might be wrong for legacy builders.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from tensorflow_datasets.core.dataset_builder.DatasetBuilder:\n",
      " |  \n",
      " |  builder_config\n",
      " |      `tfds.core.BuilderConfig` for this builder.\n",
      " |  \n",
      " |  builder_config_cls\n",
      " |      Returns the builder config class.\n",
      " |  \n",
      " |  builder_configs\n",
      " |      Returns pre-defined list of configurations for this builder class.\n",
      " |      \n",
      " |      If the builder contains multiple versions of the same config, then the key\n",
      " |      of the dict will be \"config_name:version\".\n",
      " |  \n",
      " |  code_path\n",
      " |      Returns the path to the file where the Dataset class is located.\n",
      " |      \n",
      " |      Note: As the code can be run inside zip file. The returned value is\n",
      " |      a `Path` by default. Use `tfds.core.utils.to_write_path()` to cast\n",
      " |      the path into `Path`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        path: pathlib.Path like abstraction\n",
      " |  \n",
      " |  data_dir\n",
      " |      Returns the directory where this version + config is stored.\n",
      " |      \n",
      " |      Note that this is different from `data_dir_root`. For example, if\n",
      " |      `data_dir_root` is `/data/tfds`, then `data_dir` would be\n",
      " |      `/data/tfds/my_dataset/my_config/1.2.3`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        the directory where this version + config is stored.\n",
      " |  \n",
      " |  data_dir_root\n",
      " |      Returns the root directory where all TFDS datasets are stored.\n",
      " |      \n",
      " |      Note that this is different from `data_dir`, which includes the dataset\n",
      " |      name, config, and version. For example, if `data_dir` is\n",
      " |      `/data/tfds/my_dataset/my_config/1.2.3`, then `data_dir_root` is\n",
      " |      `/data/tfds`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        the root directory where all TFDS datasets are stored.\n",
      " |  \n",
      " |  data_path\n",
      " |      Returns the path where this version + config is stored.\n",
      " |  \n",
      " |  default_builder_config\n",
      " |  \n",
      " |  info\n",
      " |      `tfds.core.DatasetInfo` for this builder.\n",
      " |  \n",
      " |  release_notes\n",
      " |  \n",
      " |  url_infos\n",
      " |      Load `UrlInfo` from the given path.\n",
      " |  \n",
      " |  version\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from tensorflow_datasets.core.dataset_builder.DatasetBuilder:\n",
      " |  \n",
      " |  BUILDER_CONFIGS = []\n",
      " |  \n",
      " |  DEFAULT_BUILDER_CONFIG_NAME = None\n",
      " |  \n",
      " |  MANUAL_DOWNLOAD_INSTRUCTIONS = None\n",
      " |  \n",
      " |  MAX_SIMULTANEOUS_DOWNLOADS = None\n",
      " |  \n",
      " |  RELEASE_NOTES = {}\n",
      " |  \n",
      " |  SUPPORTED_VERSIONS = []\n",
      " |  \n",
      " |  VERSION = None\n",
      " |  \n",
      " |  pkg_dir_path = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow_datasets.core.registered.RegisteredDataset:\n",
      " |  \n",
      " |  __init_subclass__(skip_registration=False, **kwargs) from abc.ABCMeta\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow_datasets.core.registered.RegisteredDataset:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dataset_builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e060ab8a-0b73-4da4-ae87-c47ba1bbfb14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='cobot',\n",
       "    full_name='cobot/0.0.1',\n",
       "    description=\"\"\"\n",
       "    \n",
       "    \"\"\",\n",
       "    homepage='https://www.tensorflow.org/datasets/catalog/cobot',\n",
       "    data_dir='../../tmp/cobot',\n",
       "    file_format=tfrecord,\n",
       "    download_size=Unknown size,\n",
       "    dataset_size=216.51 MiB,\n",
       "    features=FeaturesDict({\n",
       "        'steps': Dataset({\n",
       "            'action': Tensor(shape=(6,), dtype=float64),\n",
       "            'discount': float64,\n",
       "            'is_first': bool,\n",
       "            'is_last': bool,\n",
       "            'is_terminal': bool,\n",
       "            'observation': FeaturesDict({\n",
       "                'image_primary': Tensor(shape=(480, 640, 3), dtype=uint8),\n",
       "                'image_wrist': Tensor(shape=(480, 640, 3), dtype=uint8),\n",
       "                'proprio': Tensor(shape=(6,), dtype=float64),\n",
       "            }),\n",
       "            'reward': float64,\n",
       "        }),\n",
       "    }),\n",
       "    supervised_keys=None,\n",
       "    disable_shuffling=False,\n",
       "    splits={\n",
       "        'train': <SplitInfo num_examples=1, num_shards=1>,\n",
       "    },\n",
       "    citation=\"\"\"\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_builder.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3149049-7cd1-4271-ac46-19c9072db12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-13 10:16:40.512510: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-13 10:16:40.559163: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-13 10:16:40.559308: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-13 10:16:40.561189: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-13 10:16:40.561308: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-13 10:16:40.561412: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-13 10:16:40.608440: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-13 10:16:40.608581: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-13 10:16:40.608696: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-13 10:16:40.608786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8581 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:02:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset_builder.as_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccb16541-8ef9-428e-b978-e605e025a808",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee1e7e4c-440f-49b6-886e-f1eee38b2ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec={'steps': DatasetSpec({'action': TensorSpec(shape=(6,), dtype=tf.float64, name=None), 'discount': TensorSpec(shape=(), dtype=tf.float64, name=None), 'is_first': TensorSpec(shape=(), dtype=tf.bool, name=None), 'is_last': TensorSpec(shape=(), dtype=tf.bool, name=None), 'is_terminal': TensorSpec(shape=(), dtype=tf.bool, name=None), 'observation': {'image_primary': TensorSpec(shape=(480, 640, 3), dtype=tf.uint8, name=None), 'image_wrist': TensorSpec(shape=(480, 640, 3), dtype=tf.uint8, name=None), 'proprio': TensorSpec(shape=(6,), dtype=tf.float64, name=None)}, 'reward': TensorSpec(shape=(), dtype=tf.float64, name=None)}, TensorShape([]))}>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e0eb948-f80c-4b00-a32d-f4b4b6fdb6f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7b2ba5-6e46-472b-88b5-2da182851ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
